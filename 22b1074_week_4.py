# -*- coding: utf-8 -*-
"""22B1074_WEEK_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r_jMDvJWsb565ZcnWZLy4i8JCRzzeB2c

full code2
"""

import torch
from torch.utils.data import Dataset, TensorDataset, DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import math
import inspect
from dataclasses import dataclass
from transformers import AdamW
from torch.nn import CrossEntropyLoss

def AddDataset(num_samples, max_digits=3):
    data = []
    for _ in range(num_samples):
        a = str(torch.randint(0, 10**max_digits, (1,)).item()).zfill(3)
        b = str(torch.randint(0, 10**max_digits, (1,)).item()).zfill(3)
        sum_ab = str(int(a) + int(b)).zfill(4)[::-1]
        data.append((a, b, sum_ab))
    return data

dataset = AddDataset(20000)
dataset[:10]
chars="0123456789+="
#print(vocab_size)
char_to_int = {c:i for i,c in enumerate(chars)}
int_to_char = {i:c for i,c in enumerate(chars)}
PAD_TOKEN_INDEX = len(chars)
PAD_TOKEN = 'x'
char_to_int[PAD_TOKEN] = PAD_TOKEN_INDEX
int_to_char[PAD_TOKEN_INDEX] = PAD_TOKEN
vocab_size = len(char_to_int)
print(vocab_size)
print(char_to_int)
print(int_to_char)
def encode_sequence(sequence, char_to_int):
    return [char_to_int.get(char, PAD_TOKEN_INDEX) for char in sequence]
input_sequences = []
target_sequences = []

for a, b, c in dataset:
    # Format input sequence
    input_seq = f"{a}+{b}="
    # Format target sequence
    target_seq = c

    # Encode input and target sequences
    encoded_input = encode_sequence(input_seq, char_to_int)
    encoded_target = encode_sequence(target_seq, char_to_int)

    input_sequences.append(encoded_input)
    target_sequences.append(encoded_target)

#oned_list = [item for sublist in input_sequences for item in sublist]
#print(oned_list[:24])
max_length = max(max(len(seq) for seq in input_sequences), max(len(seq) for seq in target_sequences))
padded_input_sequences = [seq + [PAD_TOKEN_INDEX] * (max_length - len(seq)) for seq in input_sequences]
padded_target_sequences = [seq + [PAD_TOKEN_INDEX] * (max_length - len(seq)) for seq in target_sequences]
input_data = torch.tensor(padded_input_sequences, dtype=torch.long)
target_data = torch.tensor(padded_target_sequences, dtype=torch.long)
print(input_data[:10])
print(target_data[:5])
n = int(0.9 * len(input_data))
train_data = TensorDataset(input_data[:n], target_data[:n])
val_data = TensorDataset(input_data[n:], target_data[n:])
#print(train_data[0])
#print(final_dataset[0])
batch_size = 256  # Smaller batch size to fit in memory
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)
#train_target_data = target_data[:n]
#val_target = target_data[n:]
#print(train_loader[:2])
print(train_data[:5])
print(train_data[:5][0])
for i, (inputs, targets) in enumerate(train_loader):
    if i >= 5:
        break
    print(f"Batch {i+1}:")
    print("Inputs:", inputs[:5])  # Print the first 5 input sequences in the batch
    print("Targets:", targets[:5]) # Print the first 5 target sequences in the batch
print(len(train_loader))
torch.manual_seed(1337)
block_size = len(input_sequences[0])

class CustomMultiHeadAttention(nn.Module):
    def __init__(self, d_model, nhead, dropout=0.15):
        super(CustomMultiHeadAttention, self).__init__()
        self.d_model = d_model
        self.nhead = nhead
        self.dropout = dropout

        assert d_model % nhead == 0, "d_model must be divisible by nhead"
        self.head_dim = d_model // nhead

        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out_linear = nn.Linear(d_model, d_model)
        self.dropout_layer = nn.Dropout(dropout)

    def forward(self, query, key, value, mask=None):
        batch_size, seq_length, _ = query.size()

        # Linear projections
        Q = self.q_linear(query).view(batch_size, seq_length, self.nhead, self.head_dim).transpose(1, 2)
        K = self.k_linear(key).view(batch_size, seq_length, self.nhead, self.head_dim).transpose(1, 2)
        V = self.v_linear(value).view(batch_size, seq_length, self.nhead, self.head_dim).transpose(1, 2)

        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        if mask is not None:
            mask = mask.unsqueeze(1).unsqueeze(2)
            scores = scores.masked_fill(mask == 0, -float('inf'))
        attn = F.softmax(scores, dim=-1)
        attn = self.dropout_layer(attn)
        output = torch.matmul(attn, V)

        # Concatenate heads and apply final linear transformation
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)
        output = self.out_linear(output)

        return output

class TransformerBlock(nn.Module):
    def __init__(self, d_model, nhead, d_ff, dropout=0.15):
        super(TransformerBlock, self).__init__()
        self.attention = CustomMultiHeadAttention(d_model, nhead, dropout)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )
        self.layer_norm1 = nn.LayerNorm(d_model)
        self.layer_norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        attn_output = self.attention(x, x, x, mask)
        x = x + self.dropout(attn_output)
        x = self.layer_norm1(x)

        ff_output = self.feed_forward(x)
        x = x + self.dropout(ff_output)
        x = self.layer_norm2(x)

        return x

class CustomTransformerModel(nn.Module):
    def __init__(self, num_layers, d_model, nhead, d_ff, dropout, vocab_size):
        super(CustomTransformerModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, nhead, d_ff, dropout)
            for _ in range(num_layers)
        ])
        self.fc_out = nn.Linear(d_model, vocab_size)

    def forward(self, x, mask=None):
        x = self.embedding(x)
        for block in self.transformer_blocks:
            x = block(x, mask)
        return self.fc_out(x)

# Initialize model parameters
#vocab_size = len(chars)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CustomTransformerModel(
    num_layers=6,
    d_model=256,
    nhead=8,
    d_ff=1024,
    dropout=0.15,
    vocab_size=vocab_size
).to(device)

# Define optimizer and loss function
optimizer = optim.Adam(model.parameters(), lr=5e-5)
criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_INDEX)
# Learning rate scheduler
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

# Gradient clipping
clip_value = 1.0

num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for batch_idx, (inputs, targets) in enumerate(train_loader):
        inputs, targets = inputs.to(device), targets.to(device)
        attention_mask = (inputs != PAD_TOKEN_INDEX).float()

        # Forward pass
        outputs = model(inputs, mask=attention_mask)
        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), clip_value)
        optimizer.step()

        total_loss += loss.item()
        if batch_idx % 10 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}")

    print(f"Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss/len(train_loader):.4f}")

def predict_sum(model, input_sequence, int_to_char, max_output_length=4):
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():  # Disable gradient calculation
        input_tensor = torch.tensor(input_sequence, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension and move to device
        output_logits = model(input_tensor)  # Get model outputs
        output_logits = output_logits[:, -max_output_length:]  # Focus on the last token predictions

        # Convert logits to probabilities
        probs = torch.softmax(output_logits, dim=-1)
        # Use beam search or sampling if necessary to get diverse outputs
        predicted_indices = torch.argmax(probs, dim=-1)  # Get predicted indices

        predicted_indices = predicted_indices.squeeze().tolist()  # Convert to list
        predicted_sum = ''.join([int_to_char[idx] for idx in reversed(predicted_indices)])
        return predicted_sum

# Example usage
input_sequence = [0, 1, 2, 10, 2, 3, 4, 11]
print(predict_sum(model, input_sequence, int_to_char))

def evaluate_model(model, val_data, int_to_char, max_output_length=4):
    model.eval()
    correct_predictions = 0
    char_predictions = 0
    char_pred_num = 0

    with torch.no_grad():
        for i in range(len(val_data)):
            input_sequence = val_data[i][0].tolist()
            actual_sum = ''.join([int_to_char[idx.item()] for idx in reversed(val_data[i][1]) if idx.item() != PAD_TOKEN_INDEX])
            predicted_sum = predict_sum(model, input_sequence, int_to_char, max_output_length)

            # Print input, actual, and predicted sums
            print(f"Input: {''.join([int_to_char[idx] for idx in input_sequence])}")
            print(f"Actual: {actual_sum}")
            print(f"Predicted: {predicted_sum}")

            # Check if the full prediction matches the actual sum
            if predicted_sum == actual_sum:
                correct_predictions += 1

            # Calculate character-level accuracy
            actual_length = len(actual_sum)
            predicted_length = len(predicted_sum)
            for j in range(min(actual_length, predicted_length)):
                if predicted_sum[j] == actual_sum[j]:
                    char_predictions += 1
                char_pred_num += 1

    # Print results
    accuracy = correct_predictions
    char_accuracy = char_predictions

    print(f"Accuracy of full predictions: {accuracy} out of {char_pred_num // 4}")
    print(f"Character Level Accuracy: {char_accuracy} out of {char_pred_num}")
    #print(f"Number of wrong character positions: {char_pred_num}")

# Example usage:
evaluate_model(model, val_data, int_to_char)