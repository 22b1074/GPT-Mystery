{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "nSXrB8i8z3cw",
        "outputId": "b276a795-d136-40eb-bb88-5e99493d34c7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4d643595-dd4a-441e-a318-31ffe168329b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4d643595-dd4a-441e-a318-31ffe168329b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving names.txt to names.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the file from your local system\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "\n",
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(words[:10])\n",
        "len(words)"
      ],
      "metadata": {
        "id": "eh99fEuQ1NCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d10d999b-550c-4194-a302-7855563e8286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mappings of characters to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n"
      ],
      "metadata": {
        "id": "k1pEDSpp2bmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = torch.zeros((27, 27, 27), dtype=torch.int32)"
      ],
      "metadata": {
        "id": "xc6U18e25ShI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    N[ix1, ix2, ix3] += 1"
      ],
      "metadata": {
        "id": "FoMmtjdy6Uk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Counting Method\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "N_bigram = N.view(27*27, 27)\n",
        "P = (N_bigram + 1).float()\n",
        "print(P.sum(dim=1, keepdim=True).shape)\n",
        "print(P[54])\n",
        "P = P / P.sum(dim=1, keepdim=True)\n",
        "#P[torch.isnan(P)] = 1.0 / 27\n",
        "print(P[54])\n",
        "#print(row_sums)\n",
        "print (P[364])\n",
        "for i in range(50):\n",
        "  out = []\n",
        "  ix1 = 0\n",
        "  ix2 = 0\n",
        "  while True:\n",
        "    #px = ix % 27\n",
        "    p = P[ix1 * 27 + ix2]\n",
        "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(itos[ix])\n",
        "    if ix == 0:\n",
        "         break\n",
        "    ix1 = ix2\n",
        "    ix2 = ix\n",
        "  print(''.join(out))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXngoqGNN8ZC",
        "outputId": "f60e6e7c-0545-40c1-f327-5343e29e3aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([729, 1])\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "tensor([0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
            "        0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
            "        0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370])\n",
            "tensor([0.0154, 0.3744, 0.0051, 0.0051, 0.0051, 0.1744, 0.0051, 0.0051, 0.0051,\n",
            "        0.1897, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0308, 0.0051, 0.0051,\n",
            "        0.0154, 0.0051, 0.0051, 0.0103, 0.0051, 0.0051, 0.0051, 0.0923, 0.0051])\n",
            "junide.\n",
            "ilyasid.\n",
            "prelay.\n",
            "ocin.\n",
            "fairritoper.\n",
            "sathen.\n",
            "dannaaryanileniassibduinrwin.\n",
            "lessiyanayla.\n",
            "te.\n",
            "farmumthyfortumj.\n",
            "ponn.\n",
            "zena.\n",
            "jaylicore.\n",
            "ya.\n",
            "zoffra.\n",
            "jamilyn.\n",
            "fmouis.\n",
            "yah.\n",
            "wanaasnhavi.\n",
            "honszxhddion.\n",
            "mathani.\n",
            "zie.\n",
            "paun.\n",
            "ty.\n",
            "tin.\n",
            "rosli.\n",
            "ish.\n",
            "ubri.\n",
            "mjeaurickivina.\n",
            "ha.\n",
            "van.\n",
            "cra.\n",
            "raydnh.\n",
            "fraita.\n",
            "malyn.\n",
            "brey.\n",
            "ugi.\n",
            "zavarocbzthemiraya.\n",
            "ath.\n",
            ".\n",
            ".\n",
            "nely.\n",
            "favisotten.\n",
            "salee.\n",
            "grin.\n",
            "noew.\n",
            "fabethellianten.\n",
            "chan.\n",
            "ura.\n",
            "odridyden.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss in counting\n",
        "log_likelihood = 0.0\n",
        "n = 0\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    prob = P[ix1 * 27 + ix2, ix3]\n",
        "    logprob = torch.log(prob)\n",
        "    log_likelihood += logprob\n",
        "    n += 1\n",
        "print(f'{log_likelihood=}')\n",
        "nil = -log_likelihood\n",
        "print(f'{nil=}')\n",
        "print(n)\n",
        "print(f'{nil/n}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4OXXReNOfb_",
        "outputId": "08ad93fd-aef6-4e5b-891b-f4711372a51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log_likelihood=tensor(-410414.9688)\n",
            "nil=tensor(410414.9688)\n",
            "196113\n",
            "2.092747449874878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# neural net method\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "num_words = len(words)\n",
        "train_size = int(0.8 * num_words)\n",
        "dev_size = int(0.1 * num_words)\n",
        "train_words = words[:train_size]\n",
        "dev_words = words[train_size:train_size+dev_size]\n",
        "test_words = words[train_size+dev_size:]\n",
        "\n",
        "#function for data preparation\n",
        "def data_prep(words, stoi):\n",
        "  xs, ys = [], []\n",
        "  for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "      ix1 = stoi[ch1]\n",
        "      ix2 = stoi[ch2]\n",
        "      ix3 = stoi[ch3]\n",
        "      xs.append(ix1 *27 + ix2)\n",
        "      ys.append(ix3)\n",
        "  return torch.tensor(xs), torch.tensor(ys)\n",
        "train_xs, train_ys = data_prep(train_words, stoi)\n",
        "dev_xs, dev_ys = data_prep(dev_words, stoi)\n",
        "test_xs, test_ys = data_prep(test_words, stoi)\n",
        "full_set_xs, full_set_ys = data_prep(words, stoi)\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "intial_weights = torch.randn((27 * 27, 27), generator=g, requires_grad=True)\n",
        "\n",
        "def train_model_one_hot_softmax(xs, ys, w, reg_strength, epochs, lr):\n",
        "    w = intial_weights.clone().detach().requires_grad_(True)\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        xenc = F.one_hot(xs, num_classes=27*27).float()\n",
        "        logits = xenc @ w\n",
        "        counts = logits.exp()\n",
        "        probs = counts / counts.sum(1, keepdims=True)\n",
        "        loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
        "        # Regularization\n",
        "        reg_loss = reg_strength * (w**2).mean()\n",
        "        total_loss = loss + reg_loss\n",
        "\n",
        "        # Backward pass\n",
        "        w.grad = None  # set to 0 the gradient\n",
        "        total_loss.backward()\n",
        "\n",
        "        # Update\n",
        "        w.data += -lr * w.grad\n",
        "    return w,total_loss.item()\n",
        "\n",
        "#training model with one_hot with cross-entropy loss\n",
        "def train_model_one_hot_cross_entropy(xs, ys, w, reg_strength,epochs, lr):\n",
        "    w = intial_weights.clone().detach().requires_grad_(True)\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        xenc = F.one_hot(xs, num_classes=27*27).float()\n",
        "        logits = xenc @ w\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        loss = F.cross_entropy(logits, ys)\n",
        "        # Regularization\n",
        "        reg_loss = reg_strength * (w**2).mean()\n",
        "        total_loss = loss + reg_loss\n",
        "        # Backward pass\n",
        "        w.grad = None  # set to 0 the gradient\n",
        "        total_loss.backward()\n",
        "\n",
        "        # Update\n",
        "        w.data += -lr * w.grad\n",
        "    return w,total_loss.item()\n",
        "\n",
        "# training model with indexing into rows of w,softmax loss\n",
        "def train_model_indexing_softmax(xs, ys, w, reg_strength,epochs, lr):\n",
        "  w = intial_weights.clone().detach().requires_grad_(True)\n",
        "  for epoch in range(epochs):\n",
        "    #Forward pass\n",
        "    logits = w[xs]\n",
        "    counts = logits.exp()\n",
        "    probs = counts / counts.sum(1, keepdims=True)\n",
        "    loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
        "    #Regularization\n",
        "    reg_loss = reg_strength * (w ** 2).mean()\n",
        "    total_loss = loss + reg_loss\n",
        "    #Backward pass\n",
        "    w.grad = None  # set to 0 the gradient\n",
        "    total_loss.backward()\n",
        "    #Update\n",
        "    w.data += -lr * w.grad\n",
        "  return w,total_loss.item()\n",
        "\n",
        "# training model with indexing into rows of w,cross_entropy loss\n",
        "def train_model_indexing_cross_entropy(xs, ys, w, reg_strength,epochs, lr):\n",
        "  w = intial_weights.clone().detach().requires_grad_(True)\n",
        "  for epoch in range(epochs):\n",
        "    #Forward pass\n",
        "    logits = w[xs]\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    loss = F.cross_entropy(logits, ys)\n",
        "    #Regularization\n",
        "    reg_loss = reg_strength * (w ** 2).mean()\n",
        "    total_loss = loss + reg_loss\n",
        "    #Backward pass\n",
        "    w.grad = None\n",
        "    total_loss.backward()\n",
        "    #Update\n",
        "    w.data += -lr * w.grad\n",
        "  return w,total_loss.item()\n",
        "\n",
        "\n",
        "# new words generated based on one-hot,softmax loss\n",
        "# use w=train_w_one_hot_softmax\n",
        "def produce_words_one_hot_softmax(w, num_words=50):\n",
        "  g = torch.Generator().manual_seed(2147483647)\n",
        "  for i in range(num_words):\n",
        "    out = []\n",
        "    ix1 = 0\n",
        "    ix2 = 0\n",
        "    while True:\n",
        "      idx = ix1 * 27 + ix2\n",
        "      xenc = F.one_hot(torch.tensor([idx]), num_classes=27*27).float()\n",
        "      logits = xenc @ w\n",
        "      counts = logits.exp()\n",
        "      probs = counts / counts.sum(1, keepdims=True)\n",
        "      ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
        "      out.append(itos[ix])\n",
        "      if ix == 0:\n",
        "           break\n",
        "      ix1 = ix2\n",
        "      ix2 = ix\n",
        "    print(''.join(out))\n",
        "\n",
        "# new words generated based on one-hot,cross entropy loss\n",
        "# use w=train_w_one_hot_cross_entropy\n",
        "def produce_words_one_hot_cross_entropy(w, num_words=50):\n",
        "  g = torch.Generator().manual_seed(2147483647)\n",
        "  for i in range(num_words):\n",
        "    out = []\n",
        "    ix1 = 0\n",
        "    ix2 = 0\n",
        "    while True:\n",
        "      idx = ix1 * 27 + ix2\n",
        "      xenc = F.one_hot(torch.tensor([idx]), num_classes=27*27).float()\n",
        "      logits = xenc @ w\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
        "      out.append(itos[ix])\n",
        "      if ix == 0:\n",
        "           break\n",
        "      ix1 = ix2\n",
        "      ix2 = ix\n",
        "    print(''.join(out))\n",
        "\n",
        "# new words generated based on indexing,softmax loss\n",
        "# use w=train_w_indexing_softmax\n",
        "def produce_words_indexing_softmax(w, num_words=50):\n",
        "  g = torch.Generator().manual_seed(2147483647)\n",
        "  for i in range(num_words):\n",
        "    out = []\n",
        "    ix1 = 0\n",
        "    ix2 = 0\n",
        "    while True:\n",
        "      idx = ix1 * 27 + ix2\n",
        "      logits = w[idx]\n",
        "      counts = logits.exp()\n",
        "      probs = counts / counts.sum(0, keepdims=True)\n",
        "      ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
        "      out.append(itos[ix])\n",
        "      if ix == 0:\n",
        "           break\n",
        "      ix1 = ix2\n",
        "      ix2 = ix\n",
        "    print(''.join(out))\n",
        "\n",
        "# new words generated based on indexing,cross entropy loss\n",
        "# use w=train_w_indexing_cross_entropy\n",
        "def produce_words_indexing_cross_entropy(w, num_words=50):\n",
        "  g = torch.Generator().manual_seed(2147483647)\n",
        "  for i in range(num_words):\n",
        "    out = []\n",
        "    ix1 = 0\n",
        "    ix2 = 0\n",
        "    while True:\n",
        "      idx = ix1 * 27 + ix2\n",
        "      logits = w[idx]\n",
        "      probs = F.softmax(logits, dim=0)\n",
        "      ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
        "      out.append(itos[ix])\n",
        "      if ix == 0:\n",
        "           break\n",
        "      ix1 = ix2\n",
        "      ix2 = ix\n",
        "    print(''.join(out))\n",
        "\n",
        "# Evaluate one-hot,softmax model\n",
        "def evaluate_one_hot_softmax(xs, ys, w):\n",
        "    xenc = F.one_hot(xs, num_classes=27 * 27).float()\n",
        "    logits = xenc @ w\n",
        "    counts = logits.exp()\n",
        "    probs = counts / counts.sum(1, keepdims=True)\n",
        "    loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
        "    return loss.item()\n",
        "\n",
        "# Evaluate one-hot,cross-entropy model\n",
        "def evaluate_one_hot_cross_entropy(xs, ys, w):\n",
        "    xenc = F.one_hot(xs, num_classes=27 * 27).float()\n",
        "    logits = xenc @ w\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    loss = F.cross_entropy(logits, ys)\n",
        "    return loss.item()\n",
        "\n",
        "# Evaluate indexing,softmax model\n",
        "def evaluate_indexing_softmax(xs, ys, w):\n",
        "    logits = w[xs]\n",
        "    counts = logits.exp()\n",
        "    probs = counts / counts.sum(1, keepdims=True)\n",
        "    loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
        "    return loss.item()\n",
        "\n",
        "# Evaluate one-hot,cross-entropy model\n",
        "def evaluate_indexing_cross_entropy(xs, ys, w):\n",
        "    logits = w[xs]\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    loss = F.cross_entropy(logits, ys)\n",
        "    return loss.item()\n"
      ],
      "metadata": {
        "id": "MyS8oTlASEkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_w_one_hot_softmax,train_loss_one_hot_softmax = train_model_one_hot_softmax(train_xs, train_ys, w, reg_strength, epochs=50, lr=500)\n",
        "print(f'{train_loss_one_hot_softmax=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq4M2keJXjbs",
        "outputId": "758a21ca-50c9-4fea-b89c-0a8e084b1157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_one_hot_softmax=2.18642258644104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_w_one_hot_cross_entropy,train_loss_one_hot_cross_entropy = train_model_one_hot_cross_entropy(train_xs, train_ys, w, reg_strength, epochs=50, lr=500)\n",
        "print(f'{train_loss_one_hot_cross_entropy=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhegLVStvZNk",
        "outputId": "54a30f4d-17cc-4f94-d302-eb886a1fc740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_one_hot_cross_entropy=2.1604061126708984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_w_indexing_softmax,train_loss_indexing_softmax = train_model_indexing_softmax(train_xs, train_ys, w, reg_strength, epochs=50, lr=500)\n",
        "print(f'{train_loss_indexing_softmax=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnmSElXXwbX_",
        "outputId": "d8783114-276e-4cd7-bfe0-0165940bd6fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1624019145965576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_w_indexing_cross_entropy,train_loss_indexing_cross_entropy = train_model_indexing_cross_entropy(train_xs, train_ys, w, reg_strength, epochs=50, lr=500)\n",
        "print(f'{train_loss_indexing_cross_entropy=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxQsyHt0wBtF",
        "outputId": "b05c47f5-1afb-4876-eaec-0549df44520f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1618425846099854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_w_one_hot_softmax,dev_loss_one_hot_softmax = train_model_one_hot_softmax(dev_xs, dev_ys, w, reg_strength, epochs=50, lr=500)\n",
        "print(f'{dev_loss_one_hot_softmax=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlQLC3Wuw34E",
        "outputId": "5969489e-691c-47aa-a221-ba262742bbc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev_loss_one_hot_softmax=tensor(2.0678, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_w_one_hot_cross_entropy,dev_loss_one_hot_cross_entropy = train_model_one_hot_cross_entropy(dev_xs, dev_ys, w, reg_strength, epochs=50, lr=500)\n",
        "print(f'{dev_loss_one_hot_cross_entropy=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxpvoVRJzx9I",
        "outputId": "0100178e-45ab-456b-d4a5-d28285c198a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev_loss_one_hot_cross_entropy=2.0745949745178223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_w_indexing_softmax,dev_loss_indexing_softmax = train_model_indexing_softmax(dev_xs, dev_ys, w, reg_strength, epochs=50, lr=500)\n",
        "print(f'{dev_loss_indexing_softmax=}')"
      ],
      "metadata": {
        "id": "Nd8AR4OgxDsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_w_indexing_cross_entropy,dev_loss_indexing_cross_entropy = train_model_indexing_cross_entropy(dev_xs, dev_ys, w, reg_strength, epochs=50, lr=500)\n",
        "print(f'{dev_loss_indexing_cross_entropy=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG6_JuyOxki8",
        "outputId": "27d9113f-8a5d-4d84-ecdd-8a0d4829dfac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev_loss_indexing_cross_entropy=2.085721969604492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Regularization in one-hot ,softmax\n",
        "regularization_strengths = [0, 0.1, 0.01, 0.001]\n",
        "best_dev_loss_one_hot_softmax = float('inf')\n",
        "best_w_one_hot_softmax = None\n",
        "best_reg_strength_one_hot_softmax = None\n",
        "print('One-hot, softmax model')\n",
        "for reg_strength in regularization_strengths:\n",
        "  train_w_one_hot_softmax,train_loss_one_hot_softmax = train_model_one_hot_softmax(train_xs, train_ys, w, reg_strength, epochs=50, lr=500)\n",
        "  dev_w_one_hot_softmax,dev_loss_one_hot_softmax = train_model_one_hot_softmax(dev_xs, dev_ys, w, reg_strength, epochs=50, lr=500)\n",
        "  print(f'Regularization Strength: {reg_strength}')\n",
        "  print(f'Train Loss: {train_loss_one_hot_softmax}')\n",
        "  print(f'Dev Loss: {dev_loss_one_hot_softmax}')\n",
        "  if dev_loss_one_hot_softmax < best_dev_loss_one_hot_softmax:\n",
        "    best_dev_loss_one_hot_softmax = dev_loss_one_hot_softmax\n",
        "    best_w_one_hot_softmax = train_w_one_hot_softmax\n",
        "    best_reg_strength_one_hot_softmax = reg_strength\n",
        "print(f'{best_reg_strength_one_hot_softmax=}')\n",
        "\n",
        "test_loss_one_hot_softmax = evaluate_one_hot_softmax(test_xs, test_ys, best_w_one_hot_softmax)\n",
        "print(f'{test_loss_one_hot_softmax=}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z02b5K-xdZuJ",
        "outputId": "8379f151-e152-49f7-be5b-19640b40ad8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot, softmax model\n",
            "Regularization Strength: 0\n",
            "Train Loss: 2.161203384399414\n",
            "Dev Loss: 2.068009376525879\n",
            "Regularization Strength: 0.1\n",
            "Train Loss: 2.2520437240600586\n",
            "Dev Loss: 2.1716761589050293\n",
            "Regularization Strength: 0.01\n",
            "Train Loss: 2.190692901611328\n",
            "Dev Loss: 2.0814783573150635\n",
            "Regularization Strength: 0.001\n",
            "Train Loss: 2.161137104034424\n",
            "Dev Loss: 2.069394826889038\n",
            "best_reg_strength_one_hot_softmax=0\n",
            "test_loss_one_hot_softmax=2.18503475189209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regularization in one-hot ,cross entropy\n",
        "regularization_strengths = [0, 0.1, 0.01, 0.001]\n",
        "best_dev_loss_one_hot_cross_entropy = float('inf')\n",
        "best_w_one_hot_cross_entropy = None\n",
        "best_reg_strength_one_hot_cross_entropy = None\n",
        "print('One-hot, cross-entropy model')\n",
        "for reg_strength in regularization_strengths:\n",
        "  train_w_one_hot_cross_entropy,train_loss_one_hot_cross_entropy = train_model_one_hot_cross_entropy(train_xs, train_ys, w, reg_strength, epochs=50, lr=500)\n",
        "  dev_w_one_hot_cross_entropy,dev_loss_one_hot_cross_entropy = train_model_one_hot_cross_entropy(dev_xs, dev_ys, w, reg_strength, epochs=50, lr=500)\n",
        "  print(f'Regularization Strength: {reg_strength}')\n",
        "  print(f'Train Loss: {train_loss_one_hot_cross_entropy}')\n",
        "  print(f'Dev Loss: {dev_loss_one_hot_cross_entropy}')\n",
        "  if dev_loss_one_hot_cross_entropy < best_dev_loss_one_hot_cross_entropy:\n",
        "    best_dev_loss_one_hot_cross_entropy = dev_loss_one_hot_cross_entropy\n",
        "    best_w_one_hot_cross_entropy = train_w_one_hot_cross_entropy\n",
        "    best_reg_strength_one_hot_cross_entropy = reg_strength\n",
        "print(f'{best_reg_strength_one_hot_cross_entropy=}')\n",
        "\n",
        "test_loss_one_hot_cross_entropy = evaluate_one_hot_cross_entropy(test_xs, test_ys, best_w_one_hot_cross_entropy)\n",
        "print(f'{test_loss_one_hot_cross_entropy=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p68ZSR6jhOU4",
        "outputId": "f219e591-8493-4466-a9fe-2bb7bc3af3ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot, cross-entropy model\n",
            "Regularization Strength: 0\n",
            "Train Loss: 2.163349151611328\n",
            "Dev Loss: 2.0680091381073\n",
            "Regularization Strength: 0.1\n",
            "Train Loss: 2.2522454261779785\n",
            "Dev Loss: 2.17167592048645\n",
            "Regularization Strength: 0.01\n",
            "Train Loss: 2.1902639865875244\n",
            "Dev Loss: 2.0814781188964844\n",
            "Regularization Strength: 0.001\n",
            "Train Loss: 2.161142110824585\n",
            "Dev Loss: 2.069394826889038\n",
            "best_reg_strength_one_hot_cross_entropy=0\n",
            "test_loss_one_hot_cross_entropy=2.18947172164917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Regularization in indexing ,softmax\n",
        "regularization_strengths = [0, 0.1, 0.01, 0.001]\n",
        "best_dev_loss_indexing_softmax = float('inf')\n",
        "best_w_indexing_softmax = None\n",
        "best_reg_strength_indexing_softmax = None\n",
        "print('Indexing, softmax model')\n",
        "for reg_strength in regularization_strengths:\n",
        "  train_w_indexing_softmax,train_loss_indexing_softmax = train_model_indexing_softmax(train_xs, train_ys, w, reg_strength, epochs=50, lr=500)\n",
        "  dev_w_indexing_softmax,dev_loss_indexing_softmax = train_model_indexing_softmax(dev_xs, dev_ys, w, reg_strength, epochs=50, lr=500)\n",
        "  print(f'Regularization Strength: {reg_strength}')\n",
        "  print(f'Train Loss: {train_loss_indexing_softmax}')\n",
        "  print(f'Dev Loss: {dev_loss_indexing_softmax}')\n",
        "  if dev_loss_indexing_softmax < best_dev_loss_indexing_softmax:\n",
        "    best_dev_loss_indexing_softmax = dev_loss_indexing_softmax\n",
        "    best_w_indexing_softmax = train_w_indexing_softmax\n",
        "    best_reg_strength_indexing_softmax = reg_strength\n",
        "print(f'{best_reg_strength_indexing_softmax=}')\n",
        "\n",
        "test_loss_indexing_softmax = evaluate_indexing_softmax(test_xs, test_ys, best_w_indexing_softmax)\n",
        "print(f'{test_loss_indexing_softmax=}')"
      ],
      "metadata": {
        "id": "sYfQxZyM6Gq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64729bac-16cb-42a6-90c3-a42f4c4806c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing, softmax model\n",
            "Regularization Strength: 0\n",
            "Train Loss: 2.1612956523895264\n",
            "Dev Loss: 2.068009376525879\n",
            "Regularization Strength: 0.1\n",
            "Train Loss: 2.2549948692321777\n",
            "Dev Loss: 2.1716761589050293\n",
            "Regularization Strength: 0.01\n",
            "Train Loss: 2.177875280380249\n",
            "Dev Loss: 2.0814783573150635\n",
            "Regularization Strength: 0.001\n",
            "Train Loss: 2.1687848567962646\n",
            "Dev Loss: 2.069394826889038\n",
            "0\n",
            "test_loss_indexing_softmax=2.1788523197174072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Regularization in indexing,cross-entropy\n",
        "regularization_strengths = [0, 0.1, 0.01, 0.001]\n",
        "best_dev_loss_indexing_cross_entropy = float('inf')\n",
        "best_w_indexing_cross_entropy = None\n",
        "best_reg_strength_indexing_cross_entropy = None\n",
        "print('Indexing, cross-entropy model')\n",
        "for reg_strength in regularization_strengths:\n",
        "  train_w_indexing_cross_entropy,train_loss_indexing_cross_entropy = train_model_indexing_cross_entropy(train_xs, train_ys, w, reg_strength, epochs=50, lr=500)\n",
        "  dev_w_indexing_cross_entropy,dev_loss_indexing_cross_entropy = train_model_indexing_cross_entropy(dev_xs, dev_ys, w, reg_strength, epochs=50, lr=500)\n",
        "  print(f'Regularization Strength: {reg_strength}')\n",
        "  print(f'Train Loss: {train_loss_indexing_cross_entropy}')\n",
        "  print(f'Dev Loss: {dev_loss_indexing_cross_entropy}')\n",
        "  if dev_loss_indexing_cross_entropy < best_dev_loss_indexing_cross_entropy:\n",
        "    best_dev_loss_indexing_cross_entropy = dev_loss_indexing_cross_entropy\n",
        "    best_w_indexing_cross_entropy = train_w_indexing_cross_entropy\n",
        "    best_reg_strength_indexing_cross_entropy = reg_strength\n",
        "print(f'{best_reg_strength_indexing_cross_entropy}')\n",
        "\n",
        "test_loss_indexing_cross_entropy = evaluate_indexing_cross_entropy(test_xs, test_ys, best_w_indexing_cross_entropy)\n",
        "print(f'{test_loss_indexing_cross_entropy=}')"
      ],
      "metadata": {
        "id": "LId2r4Ut6ROC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d094a2-0bd5-4913-9a97-655c1958339f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing, cross-entropy model\n",
            "Regularization Strength: 0\n",
            "Train Loss: 2.1612956523895264\n",
            "Dev Loss: 2.0680091381073\n",
            "Regularization Strength: 0.1\n",
            "Train Loss: 2.254995346069336\n",
            "Dev Loss: 2.1716761589050293\n",
            "Regularization Strength: 0.01\n",
            "Train Loss: 2.1778059005737305\n",
            "Dev Loss: 2.0814783573150635\n",
            "Regularization Strength: 0.001\n",
            "Train Loss: 2.1676249504089355\n",
            "Dev Loss: 2.069394826889038\n",
            "0\n",
            "test_loss_indexing_cross_entropy=2.178852081298828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "produce_words_one_hot_softmax(best_w_one_hot_softmax)"
      ],
      "metadata": {
        "id": "Hn2uaI9zRvJi",
        "outputId": "d1551e23-87c0-4ed1-f09b-8c5ff89eb959",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ouwade.\n",
            ".\n",
            "osaqah.\n",
            "prelay.\n",
            "ocnzi.\n",
            "orin.\n",
            "tonia.\n",
            "oster.\n",
            "yulinaaryanileniassibduinrwibel.\n",
            "se.\n",
            "yinayla.\n",
            "te.\n",
            "yarmumtrifoe.\n",
            "umj.\n",
            "phyashiah.\n",
            "zaylicora.\n",
            "ya.\n",
            "zocken.\n",
            "yandreerimouim.\n",
            "yfvn.\n",
            "thaasnhmvfjfopszxhxdgorfmxtyn.\n",
            "il.\n",
            ".\n",
            "ree.\n",
            "olany.\n",
            "tin.\n",
            "yosli.\n",
            "isamey.\n",
            "yumjlkujcmkhaubwyhepharccwarriyan.\n",
            "na.\n",
            "zita.\n",
            "patyrabel.\n",
            "ha.\n",
            "ivellarocbzzhqmfwveawath.\n",
            ".\n",
            ".\n",
            "odxsktqvslettkczsalee.\n",
            "prin.\n",
            "noewkflbjtnellianten.\n",
            "ya.\n",
            "yeusfso.\n",
            "yodcdpqajbaliypjrgia.\n",
            "tezrwaylia.\n",
            "oywhqelvin.\n",
            "osa.\n",
            "omihaurik.\n",
            "tim.\n",
            ".\n",
            "ossabdon.\n",
            "oli.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "produce_words_one_hot_cross_entropy(best_w_one_hot_cross_entropy)"
      ],
      "metadata": {
        "id": "vgpdNuEu2buk",
        "outputId": "eeb9b5d6-ffec-49d0-e857-45bfd5fa33db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ouwade.\n",
            ".\n",
            "osaqah.\n",
            "prelay.\n",
            "ocnzi.\n",
            "orin.\n",
            "tonia.\n",
            "oster.\n",
            "yulinaaryanileniassibduinrwibel.\n",
            "se.\n",
            "yinayla.\n",
            "te.\n",
            "yarmumtrifoe.\n",
            "umj.\n",
            "phyashiah.\n",
            "zaylicora.\n",
            "ya.\n",
            "zocken.\n",
            "yandreerimouim.\n",
            "yfvn.\n",
            "thaasnhmvfjfopszxhxdgorfmxtyn.\n",
            "il.\n",
            ".\n",
            "ree.\n",
            "olan.\n",
            "ot.\n",
            "ryn.\n",
            "zai.\n",
            "isamey.\n",
            "yumjlkujcmkhaubwyhepharccwarriyan.\n",
            "na.\n",
            "zita.\n",
            "patyrabel.\n",
            "ha.\n",
            "ivellarocbzzhqmfwveawath.\n",
            ".\n",
            ".\n",
            "odxsktqvslettkczsalee.\n",
            "prin.\n",
            "noewkflbjtnellianten.\n",
            "ya.\n",
            "yeusfso.\n",
            "yodcdpqajbaliypjrgia.\n",
            "tezrwaylia.\n",
            "oywhqelvin.\n",
            "osa.\n",
            "omihaurik.\n",
            "tim.\n",
            ".\n",
            "ossabdon.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "produce_words_indexing_softmax(best_w_indexing_softmax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiJ1pIEe4lh9",
        "outputId": "fca67651-30de-4536-c735-7d371a044052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ouwade.\n",
            ".\n",
            "osaqah.\n",
            "prelay.\n",
            "ocnzi.\n",
            "orin.\n",
            "tonia.\n",
            "oster.\n",
            "yulinaaryanileniassibduinrwibel.\n",
            "se.\n",
            "yinayla.\n",
            "te.\n",
            "yarmumtrifoe.\n",
            "umj.\n",
            "phyashiah.\n",
            "zaylicora.\n",
            "ya.\n",
            "zocken.\n",
            "yandreerimouim.\n",
            "yfvn.\n",
            "thaasnhmvfjfopszxhxdgorfmxtyn.\n",
            "il.\n",
            ".\n",
            "ree.\n",
            "olany.\n",
            "tin.\n",
            "yosli.\n",
            "isamey.\n",
            "yumjlkujcmkhaubwyhepharccwarriyanhwannita.\n",
            "patyrabel.\n",
            "ha.\n",
            "ivellarocbzzhqmfwveawath.\n",
            ".\n",
            ".\n",
            "odxsktqvslettkczsalee.\n",
            "prin.\n",
            "noewkflbjtnellianten.\n",
            "ya.\n",
            "yeusfso.\n",
            "yodcdpqajbaliypjrgia.\n",
            "tezrwaylia.\n",
            "oywhqelvin.\n",
            "osa.\n",
            "omihaurik.\n",
            "tim.\n",
            ".\n",
            "ossabdon.\n",
            "oli.\n",
            "na.\n",
            "prettwbjdgbannyah.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "produce_words_indexing_cross_entropy(best_w_indexing_cross_entropy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjAeBgIm4rN5",
        "outputId": "880ad3b6-6b96-4576-b0c8-168d2a038420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ouwade.\n",
            ".\n",
            "osaqah.\n",
            "prelay.\n",
            "ocnzi.\n",
            "orin.\n",
            "tonia.\n",
            "oster.\n",
            "yulinaaryanileniassibduinrwibel.\n",
            "se.\n",
            "yinayla.\n",
            "te.\n",
            "yarmumtrifoe.\n",
            "umj.\n",
            "phyashiah.\n",
            "zaylicora.\n",
            "ya.\n",
            "zocken.\n",
            "yandreerimouim.\n",
            "yfvn.\n",
            "thaasnhmvfjfopszxhxdgorfmxtyn.\n",
            "il.\n",
            ".\n",
            "ree.\n",
            "olany.\n",
            "tin.\n",
            "yosli.\n",
            "isamey.\n",
            "yumjlkujcmkhaubwyhepharccwarriyanhwannita.\n",
            "patyrabel.\n",
            "ha.\n",
            "ivellarocbzzhqmfwveawath.\n",
            ".\n",
            ".\n",
            "odxsktqvslettkczsalee.\n",
            "prin.\n",
            "noewkflbjtnellianten.\n",
            "ya.\n",
            "yeusfso.\n",
            "yodcdpqajbaliypjrgia.\n",
            "tezrwaylia.\n",
            "oywhqelvin.\n",
            "osa.\n",
            "omihaurik.\n",
            "tim.\n",
            ".\n",
            "ossabdon.\n",
            "oli.\n",
            "na.\n",
            "prettwbjdgbannyah.\n"
          ]
        }
      ]
    }
  ]
}